# Airflow on Kubernetes

What is Airflow?  [Airflow](https://airflow.apache.org/) is created/maintained by Apache.  In their words "Airflow is a platform created by the community to programmatically author, schedule and monitor workflows."

That's about it, it's used to kick off a workflow and to ensure that items get performed in a specific order.  The complex workflow can be placed into something called a DAG (Directed Acyclic Graph).  The DAG has lots of tasks inside of it that depend on each other and the specified order.  What's nice about Airflow is that it has a nice UI that can be used to visualize the DAG's, and previously run tasks.

Think about the DAG as a graph that describes the order of the tasks execution.  The tasks are simple functions that will be run by a worker pod and when it finishes it will have a success or fail.  The most common task values are:
- BashOperator
- PythonOperator
- KubernetesPodOperator

Let's take a look at how Airflow works using the Kubernetes Executor.

![Airflow Overflow](https://raw.githubusercontent.com/randysimpson/tools/master/k8s/airflow/overview.png)

The web and the scheduler airflow pods both need to be able to communicate throught the network to the postgres and the localstack deployments.

When the scheduler finds that a DAG should be run it will communicate with Kubernetes cluster using the ServiceAccount created and generate worker pods for each task when necessary according to the workflow.  These worker pods have the airflow image and the dags in a similar fashon as the original web and scheduler pods, but the worker pods could contain extra environment variables or other things if necessary.  The worker pod will then run the python function, or bash function it is supposed to.  If the worker pod is a KubernetesPodOperator it will create another pod using the ServiceAccount to run the specified docker image.

Now lets setup a development environment so that we can start to get the hang of Airflow.

To setup airflow on K8s we need to have some base infrastructure setup.  Obviously you will need to have a k8s cluster running.  If you don't have that setup please see [Setup a Kubernetes master server](https://simpsonhouse.hopto.org/blog/Setup%20a%20Kubernetes%20master%20server).

This airflow setup will include the following deployments:
* postgres -  It's recommended to use an external postgres database but for this development setup we are going to include the postgres deployment in the yaml.
* localstack - This container will allow us to simulate saving the logs to an s3 bucket.
* airflow scheduler - This is what actually kicks off the airflow worker pods to perform the workflow.
* airflow web - This is what shows the UI and can be used to trigger the dag's or perform airflow commands.

Also we are going to expose the postgres and localstack internally to the k8's cluster only.  We will expose the airflow UI as nodeport so that we can view the UI from any node in the k8's cluster.  We could have also used a loadbalancer but since my kubernetes setup is on a VM, nodeport will suffice.

There are a few things that you might want to customize for your local deployment.
1. fernet key

    The fernet key can be generated by running:
    ```
    $ pip install cryptography

    $ python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
    ```
    Then you should create a kuberentes secret using that value by issuing the following command:
    ```
    $ kubectl create secret generic airflow-fernet --from-literal=AIRFLOW__CORE__FERNET_KEY=<output_from_last_command>
    ```
2. postgres secret

    The postgres secret can be generated by issuing the following command, and putting in your custom values for `<password>`, `<username>`:
    ```
    $ kubectl create secret generic airflow-postgres --from-literal=DATABASE_PASSWORD=<password> --from-literal=DATABASE_USER=<username> --from-literal=DATABASE_DB=airflow --from-literal=DATABASE_HOST=airflow-pg --from-literal=DATABASE_PORT=5432
    ```
3. create airflow environment

    Now create the airflow infrastructure by issuing the following command:
    ```
    $ kubectl create -f https://raw.githubusercontent.com/randysimpson/tools/master/k8s/airflow/local_airflow.yaml
    ```

Or if you would rather just create all the infra without customizing the passwords then issue the command:
```
$ kubectl create -f https://raw.githubusercontent.com/randysimpson/tools/master/k8s/airflow/local_complete_airflow.yaml
```

The yaml produced is based off the helm chart found at https://github.com/airflow-helm/charts/tree/main/charts/airflow version v7.14.0.  If you want to learn more about how to work with helm charts take a look at the blog on [helm](https://simpsonhouse.hopto.org/blog/Helm).

The airflow UI is exposed as `NodePort` therefore it is exposed on every node in the k8s cluster on a specific port.  To find what port it's exposed on use `kubectl get svc airflow-web`:

```
$ kubectl get svc airflow-web
NAME          TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
airflow-web   NodePort   10.105.198.41   <none>        8080:30491/TCP   7m17s
```

In my example it can be found on port 30491.  I'll use the worker node to access the UI so it is http://k8worker1:30491

![Airflow UI](https://raw.githubusercontent.com/randysimpson/tools/master/k8s/airflow/airflow_ui.png)


### Licence

MIT License

Copyright (Â©) 2021 - Randall Simpson

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.